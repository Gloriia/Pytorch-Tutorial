{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Pytorch With Examples\n",
    "\n",
    "这份tutorial里面介绍了PyTorch中的基本概念，通过self-contained的例子。  \n",
    "里面的核心是，PyTorch提供了两个features：    \n",
    "1.一个n维的Tensor，跟numpy相同，但是可以在GPUs上运行。  \n",
    "2.自动偏微分用来建立和训练神经网络。 \n",
    "\n",
    "我们将会使用一个全连接的ReLU神经网络，这个神经网络会有单层的隐藏层，通过最小化预测output和真实ouput间的Euclidean距离，使用梯度下降来拟合随机数据。\n",
    "\n",
    "### Tensor\n",
    "### warm-up Numpy\n",
    "\n",
    "在介绍PyTorch之前，我们首先通过numpy来实现神经网络。  \n",
    "Numpy提供了一个n维数组对象，以及很多的functions来操作这些对象。Numpy是一个用于科学计算的普通框架，其中不包含任何计算图、深度学习和梯度下降。然而，我们可以很容易的用numpy来拟合随机数据中的两层神经网络，通过人为地使用numpy中的操作来实现神经网络中forward和backward的传导。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26103590.064815007\n",
      "1 23805273.064857863\n",
      "2 26517348.65556434\n",
      "3 30887937.04698555\n",
      "4 32984005.869947895\n",
      "5 29270028.258647274\n",
      "6 20642149.12989001\n",
      "7 11743797.427902006\n",
      "8 5961543.015416406\n",
      "9 3050507.243194424\n",
      "10 1751967.546260971\n",
      "11 1168103.0738413478\n",
      "12 880534.3068067222\n",
      "13 716554.0658200237\n",
      "14 607530.1942493559\n",
      "15 526274.7633681342\n",
      "16 461391.4232553778\n",
      "17 407535.013027256\n",
      "18 361886.5653740581\n",
      "19 322707.6775668518\n",
      "20 288824.1733967946\n",
      "21 259326.32431755072\n",
      "22 233523.5503189772\n",
      "23 210896.7822708857\n",
      "24 190954.83593800035\n",
      "25 173295.1519419413\n",
      "26 157611.4589557179\n",
      "27 143643.22587169393\n",
      "28 131170.9246054735\n",
      "29 120004.69315965727\n",
      "30 109981.65415377416\n",
      "31 100963.14500437843\n",
      "32 92840.66693073738\n",
      "33 85502.83187092925\n",
      "34 78859.37269018675\n",
      "35 72835.77237246887\n",
      "36 67368.51478877381\n",
      "37 62391.14551244213\n",
      "38 57854.19039917317\n",
      "39 53708.27493851545\n",
      "40 49916.57115363956\n",
      "41 46445.70428442887\n",
      "42 43260.18987011299\n",
      "43 40335.84842312551\n",
      "44 37649.880808069036\n",
      "45 35176.17400418781\n",
      "46 32896.535240222016\n",
      "47 30791.54383350408\n",
      "48 28846.22276214777\n",
      "49 27048.419538804665\n",
      "50 25383.56879309971\n",
      "51 23840.220520161776\n",
      "52 22408.794005638178\n",
      "53 21078.731456785896\n",
      "54 19842.48810075279\n",
      "55 18692.614053471236\n",
      "56 17621.800994389872\n",
      "57 16625.264006622314\n",
      "58 15696.728059173356\n",
      "59 14829.901613523936\n",
      "60 14019.883236512029\n",
      "61 13262.17470090868\n",
      "62 12552.933417266704\n",
      "63 11888.680430321343\n",
      "64 11265.977130018777\n",
      "65 10681.438107354454\n",
      "66 10132.804882390952\n",
      "67 9617.42758613878\n",
      "68 9132.908093698516\n",
      "69 8677.218504162978\n",
      "70 8248.474085710131\n",
      "71 7844.737016411759\n",
      "72 7464.151871395034\n",
      "73 7105.214963013412\n",
      "74 6766.6576844376\n",
      "75 6447.202694577631\n",
      "76 6145.366244519988\n",
      "77 5860.063601757908\n",
      "78 5590.267251515952\n",
      "79 5335.121714381639\n",
      "80 5093.530580470486\n",
      "81 4864.7335555359605\n",
      "82 4647.843395103225\n",
      "83 4442.199653863393\n",
      "84 4247.165975218752\n",
      "85 4062.065326783155\n",
      "86 3886.3801868798587\n",
      "87 3719.4823228662117\n",
      "88 3560.863242670558\n",
      "89 3410.0298140769482\n",
      "90 3266.5605932954536\n",
      "91 3130.039417897268\n",
      "92 3000.071328836423\n",
      "93 2876.3253890016204\n",
      "94 2758.4027630534983\n",
      "95 2646.13318927422\n",
      "96 2539.1079446070416\n",
      "97 2437.038097605755\n",
      "98 2339.645132584483\n",
      "99 2246.688054111828\n",
      "100 2157.9091333730066\n",
      "101 2073.0902574221154\n",
      "102 1992.0615104486624\n",
      "103 1914.612103383312\n",
      "104 1840.54473847137\n",
      "105 1769.7280946291487\n",
      "106 1701.985208775308\n",
      "107 1637.1322041356195\n",
      "108 1575.0510787257426\n",
      "109 1515.6072953858213\n",
      "110 1458.668567680106\n",
      "111 1404.1146318057902\n",
      "112 1351.8382093402965\n",
      "113 1301.7346171158276\n",
      "114 1253.6930666941048\n",
      "115 1207.610383128981\n",
      "116 1163.396141537317\n",
      "117 1120.9697912960987\n",
      "118 1080.2559034322012\n",
      "119 1041.1642371208122\n",
      "120 1003.6441672056299\n",
      "121 967.5994857255814\n",
      "122 932.9744300796568\n",
      "123 899.7038057114851\n",
      "124 867.7284832806004\n",
      "125 837.0069315187006\n",
      "126 807.4605517304697\n",
      "127 779.0523750051691\n",
      "128 751.7377292775057\n",
      "129 725.4573002279889\n",
      "130 700.178257113909\n",
      "131 675.8576096209858\n",
      "132 652.4446883131475\n",
      "133 629.9093156047445\n",
      "134 608.2119919931304\n",
      "135 587.315533504617\n",
      "136 567.1924382881938\n",
      "137 547.8128579396582\n",
      "138 529.1421724436923\n",
      "139 511.1577266854349\n",
      "140 493.82503013069424\n",
      "141 477.1234059990108\n",
      "142 461.02321132553993\n",
      "143 445.50463509186386\n",
      "144 430.54669151215137\n",
      "145 416.1254793928081\n",
      "146 402.2189682644354\n",
      "147 388.8078961471727\n",
      "148 375.86949847640494\n",
      "149 363.38763014931976\n",
      "150 351.3453717216509\n",
      "151 339.7256325815701\n",
      "152 328.51646919772486\n",
      "153 317.69498099845487\n",
      "154 307.2490810824611\n",
      "155 297.16763572767763\n",
      "156 287.4355849032936\n",
      "157 278.03820733278314\n",
      "158 268.96502356129196\n",
      "159 260.20276811447934\n",
      "160 251.74183019749182\n",
      "161 243.57053720824086\n",
      "162 235.67788585289335\n",
      "163 228.0541983918551\n",
      "164 220.68963289324148\n",
      "165 213.5728765917408\n",
      "166 206.69706199476786\n",
      "167 200.0543532278356\n",
      "168 193.63351625205001\n",
      "169 187.4296829331032\n",
      "170 181.43179564673756\n",
      "171 175.6347309710907\n",
      "172 170.0322022132836\n",
      "173 164.61516732917923\n",
      "174 159.37791013295077\n",
      "175 154.31412147962223\n",
      "176 149.41910431829305\n",
      "177 144.6861896907966\n",
      "178 140.10836613467453\n",
      "179 135.6821727271017\n",
      "180 131.4003634104701\n",
      "181 127.25901670460479\n",
      "182 123.2540352894022\n",
      "183 119.37942643752203\n",
      "184 115.6309441682055\n",
      "185 112.00503255893214\n",
      "186 108.49684971956385\n",
      "187 105.10313741227455\n",
      "188 101.81900336235495\n",
      "189 98.6409197568589\n",
      "190 95.56634333518454\n",
      "191 92.591148252551\n",
      "192 89.71258827989143\n",
      "193 86.92588042429873\n",
      "194 84.22856749938647\n",
      "195 81.61721714520506\n",
      "196 79.0903698371061\n",
      "197 76.64381115788889\n",
      "198 74.27535719513298\n",
      "199 71.98261272499374\n",
      "200 69.76322796276227\n",
      "201 67.6145347089238\n",
      "202 65.53369973216184\n",
      "203 63.51921007264528\n",
      "204 61.56837092564004\n",
      "205 59.680220587019626\n",
      "206 57.85112220960826\n",
      "207 56.079964033293045\n",
      "208 54.364332556961244\n",
      "209 52.703391846218764\n",
      "210 51.09423747053523\n",
      "211 49.5358386353789\n",
      "212 48.02612903274443\n",
      "213 46.56413132189114\n",
      "214 45.148386954424446\n",
      "215 43.77633957511199\n",
      "216 42.447041420707116\n",
      "217 41.1595604416977\n",
      "218 39.912223538623905\n",
      "219 38.70377420046894\n",
      "220 37.532838223801846\n",
      "221 36.39845340668754\n",
      "222 35.29931034812449\n",
      "223 34.23491480679414\n",
      "224 33.20340107718998\n",
      "225 32.20392382652451\n",
      "226 31.23547660668814\n",
      "227 30.29679209039633\n",
      "228 29.38715972405917\n",
      "229 28.505521927553666\n",
      "230 27.65125791354175\n",
      "231 26.82305185070661\n",
      "232 26.020450820615345\n",
      "233 25.242376004695043\n",
      "234 24.48838402080613\n",
      "235 23.757459414629132\n",
      "236 23.048907679561395\n",
      "237 22.361912983921023\n",
      "238 21.696075724050683\n",
      "239 21.05043592503276\n",
      "240 20.42445629421914\n",
      "241 19.817651872627053\n",
      "242 19.229314023434824\n",
      "243 18.658834005339497\n",
      "244 18.105758465082076\n",
      "245 17.56942135790697\n",
      "246 17.04945227943901\n",
      "247 16.54513223957973\n",
      "248 16.056134455097244\n",
      "249 15.581985623664794\n",
      "250 15.122144677002662\n",
      "251 14.676205689939765\n",
      "252 14.24368978525349\n",
      "253 13.824321415990223\n",
      "254 13.417530345378289\n",
      "255 13.023089913975998\n",
      "256 12.640599645152761\n",
      "257 12.26948050423207\n",
      "258 11.909464727471551\n",
      "259 11.560278866511727\n",
      "260 11.221568024272123\n",
      "261 10.893020095919178\n",
      "262 10.574285832483495\n",
      "263 10.265077777200345\n",
      "264 9.965193821656115\n",
      "265 9.674208412147156\n",
      "266 9.391923994479374\n",
      "267 9.118099085052979\n",
      "268 8.852421447390077\n",
      "269 8.594626693285855\n",
      "270 8.34452044153805\n",
      "271 8.101888655478952\n",
      "272 7.866425127773838\n",
      "273 7.637957702122364\n",
      "274 7.416319127394314\n",
      "275 7.201305037983267\n",
      "276 6.99252735256851\n",
      "277 6.7899445472116575\n",
      "278 6.593376238157326\n",
      "279 6.402593966322996\n",
      "280 6.217448305467041\n",
      "281 6.037788013968598\n",
      "282 5.863426813584237\n",
      "283 5.694192331814294\n",
      "284 5.529958614795461\n",
      "285 5.370568113831505\n",
      "286 5.215861729987413\n",
      "287 5.065698984849143\n",
      "288 4.919978222833008\n",
      "289 4.778507293190741\n",
      "290 4.641191802478087\n",
      "291 4.507918840863853\n",
      "292 4.378542775201567\n",
      "293 4.252993077304157\n",
      "294 4.131111178910778\n",
      "295 4.0127681296766555\n",
      "296 3.8978730319837913\n",
      "297 3.786332469612086\n",
      "298 3.6780802903878422\n",
      "299 3.572962094014517\n",
      "300 3.47090329686002\n",
      "301 3.3718312954437852\n",
      "302 3.2756335771438243\n",
      "303 3.1822366823339743\n",
      "304 3.09155995064195\n",
      "305 3.003520918689252\n",
      "306 2.9180287481938345\n",
      "307 2.835021314964411\n",
      "308 2.754433659691194\n",
      "309 2.6761658300682836\n",
      "310 2.6001797208277755\n",
      "311 2.5264143031630306\n",
      "312 2.45475197821378\n",
      "313 2.385163455662408\n",
      "314 2.3175934155966393\n",
      "315 2.2519625487347037\n",
      "316 2.1882236655942906\n",
      "317 2.126333293348398\n",
      "318 2.0662234580852137\n",
      "319 2.007841048974315\n",
      "320 1.9511464974331767\n",
      "321 1.8960745554169747\n",
      "322 1.842585963213745\n",
      "323 1.790639462138433\n",
      "324 1.7401817565830782\n",
      "325 1.691170962795682\n",
      "326 1.6435708992909686\n",
      "327 1.5973480359656362\n",
      "328 1.5524473541070591\n",
      "329 1.5088220912448063\n",
      "330 1.4664417289550589\n",
      "331 1.425271889312262\n",
      "332 1.3852823074237228\n",
      "333 1.3464332802291215\n",
      "334 1.3086931596730929\n",
      "335 1.272033204391932\n",
      "336 1.2364143306782063\n",
      "337 1.2018122602342896\n",
      "338 1.1681996991959904\n",
      "339 1.1355391051184254\n",
      "340 1.1038068663017548\n",
      "341 1.0729806139599112\n",
      "342 1.0430273679830315\n",
      "343 1.0139330003643996\n",
      "344 0.9856666040039956\n",
      "345 0.9581909633778244\n",
      "346 0.9314955026534144\n",
      "347 0.9055617242479355\n",
      "348 0.8803581738907147\n",
      "349 0.8558694655230474\n",
      "350 0.8320731975670408\n",
      "351 0.8089480704854576\n",
      "352 0.7864768774457931\n",
      "353 0.7646415322445228\n",
      "354 0.7434216752707823\n",
      "355 0.7228012034201772\n",
      "356 0.7027640119036904\n",
      "357 0.6832893118527699\n",
      "358 0.6643667997516736\n",
      "359 0.6459816355853422\n",
      "360 0.6281050810504824\n",
      "361 0.6107338080717368\n",
      "362 0.5938487747573487\n",
      "363 0.5774378415462573\n",
      "364 0.561490178654504\n",
      "365 0.5459882207692605\n",
      "366 0.5309220170092641\n",
      "367 0.5162793085463342\n",
      "368 0.5020450582072241\n",
      "369 0.48821059574190107\n",
      "370 0.4747629167196658\n",
      "371 0.461690950445758\n",
      "372 0.4489853073723663\n",
      "373 0.4366371214373785\n",
      "374 0.42463588165104393\n",
      "375 0.4129667458775126\n",
      "376 0.40162157661986053\n",
      "377 0.3905930084637322\n",
      "378 0.379872766621219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379 0.3694502256396369\n",
      "380 0.3593188038532833\n",
      "381 0.34946951626711875\n",
      "382 0.33989368355495736\n",
      "383 0.3305850648239625\n",
      "384 0.3215349616897558\n",
      "385 0.3127363406679033\n",
      "386 0.30418280146290455\n",
      "387 0.2958657421046711\n",
      "388 0.28778427654711136\n",
      "389 0.27992283625499464\n",
      "390 0.27227833201782664\n",
      "391 0.2648464408137769\n",
      "392 0.25761993156333307\n",
      "393 0.25059349136091724\n",
      "394 0.24376287055898227\n",
      "395 0.23711969255571666\n",
      "396 0.23066024107796648\n",
      "397 0.22437960395598378\n",
      "398 0.21827197328939368\n",
      "399 0.21233360427180106\n",
      "400 0.20655881779313107\n",
      "401 0.2009437041935925\n",
      "402 0.19548612568320894\n",
      "403 0.19017592526447935\n",
      "404 0.18501204657115408\n",
      "405 0.17999021725107178\n",
      "406 0.17510651425858864\n",
      "407 0.17035770042667553\n",
      "408 0.1657388200992652\n",
      "409 0.16124700065975722\n",
      "410 0.15687896640609078\n",
      "411 0.15263032664653958\n",
      "412 0.14849883690664822\n",
      "413 0.1444803336017757\n",
      "414 0.1405721545074632\n",
      "415 0.13677294273474214\n",
      "416 0.13307620613174576\n",
      "417 0.12948074658425504\n",
      "418 0.12598350979502085\n",
      "419 0.12258179247096797\n",
      "420 0.1192734571038664\n",
      "421 0.11605538167707671\n",
      "422 0.11292550818875452\n",
      "423 0.10988105953689331\n",
      "424 0.10691948342666742\n",
      "425 0.10403926413984121\n",
      "426 0.10123716469122322\n",
      "427 0.09851165256399684\n",
      "428 0.09586181482007333\n",
      "429 0.09328280772390661\n",
      "430 0.09077419461546685\n",
      "431 0.08833376636039292\n",
      "432 0.08595975055198443\n",
      "433 0.08365050657592814\n",
      "434 0.08140386744739525\n",
      "435 0.07921846097280111\n",
      "436 0.07709224161531969\n",
      "437 0.07502390948555937\n",
      "438 0.07301174890990565\n",
      "439 0.07105411022429559\n",
      "440 0.06915043265894777\n",
      "441 0.0672982056899663\n",
      "442 0.06549578342849005\n",
      "443 0.06374209273044158\n",
      "444 0.06203583102585192\n",
      "445 0.060375875982114274\n",
      "446 0.05876076688327017\n",
      "447 0.05718947858637204\n",
      "448 0.0556607084481479\n",
      "449 0.054173239585995535\n",
      "450 0.05272610555481144\n",
      "451 0.05131787331621963\n",
      "452 0.04994803935167358\n",
      "453 0.0486154370825192\n",
      "454 0.04731827360425217\n",
      "455 0.046056133522230376\n",
      "456 0.04482798814113255\n",
      "457 0.043633096277665276\n",
      "458 0.04247034102712066\n",
      "459 0.04133904240108185\n",
      "460 0.040238110366996746\n",
      "461 0.039166763357569793\n",
      "462 0.038124335769077776\n",
      "463 0.03710986625309951\n",
      "464 0.03612291972317833\n",
      "465 0.03516264903498659\n",
      "466 0.03422788546833759\n",
      "467 0.03331823310006098\n",
      "468 0.03243295963634866\n",
      "469 0.031571543912176314\n",
      "470 0.030733142453917475\n",
      "471 0.02991734735462416\n",
      "472 0.02912335294242139\n",
      "473 0.02835067362745836\n",
      "474 0.027598719371472954\n",
      "475 0.026866955348610132\n",
      "476 0.026155151836093696\n",
      "477 0.025462089155313436\n",
      "478 0.024787537532244455\n",
      "479 0.02413101117432511\n",
      "480 0.023492040339093596\n",
      "481 0.022870172140659853\n",
      "482 0.02226488563602068\n",
      "483 0.021675858275550738\n",
      "484 0.02110250197250256\n",
      "485 0.02054449897607858\n",
      "486 0.02000136620697172\n",
      "487 0.019472874627392556\n",
      "488 0.01895854597456204\n",
      "489 0.0184577628740749\n",
      "490 0.017970355651560882\n",
      "491 0.01749593424573036\n",
      "492 0.01703415295071903\n",
      "493 0.01658465344619501\n",
      "494 0.016147162305333057\n",
      "495 0.01572129215011618\n",
      "496 0.015306749421766075\n",
      "497 0.014903270524530944\n",
      "498 0.01451057820190279\n",
      "499 0.014128416029846786\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch：Tensor\n",
    "\n",
    "Numpy是一个很好的框架，但是它不能高效利用GPUs来加速科学计算。对于现代深度神经网络来说，GPUs经常可以提供50x以上的加速度，numpy对于深度神经网络来说是不够的。在这里我们引入最基本的PyTorch概念：Tensor，PyTorch Tensor概念上与numpy array类似，一个Tensor就是一个n维数组，PyTorch提供很多的函数来操作这些Tensor。除此以外，Tensors不仅可以跟计算图、梯度下降同步，还可以作为科学计算非常有效的工具。\n",
    "\n",
    "跟numpy不一样的是，PyTorch Tensors可以使用GPUs来加速数值计算，在GPUs上运行PyTorch Tensor，你只需要简单地把它转换成新的数据类型。\n",
    "\n",
    "以下是我们如何使用两层神经网络来拟合随机数据，如同以上的numpy小例子，我们需要人为地使用forward and backward来训练网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 734.4877319335938\n",
      "199 5.479307651519775\n",
      "299 0.06713082641363144\n",
      "399 0.001289833802729845\n",
      "499 0.0001208614485221915\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd  \n",
    "### PyTorch: Tensors and autograd  \n",
    "\n",
    "在上述的例子里，我们在神经网络中使用了forward and backward，使用backward在两层小神经网络中不是什么大问题，但是在大型复杂网络中就会变得很棘手。\n",
    "\n",
    "幸运地，我们使用自动偏微分在神经网络中自动计算backward，PyTorch中autograd库自动完成上述操作。在使用autograd时，神经网络的forward会被定义成计算图，图中的结点即是Tensors，图中的边使input Tensors生成output Tensors，通过计算图的Backpropagating让你可以轻易地计算出gradients。\n",
    "\n",
    "听上去很复杂，实际上很简单。每一个节点代表计算图中的Tensor，如果x是一个Tensor，然后x.requires_grad=True，那么x.grad是另一个Tensor，是x经过gradient之后调节到的另外一些scalar的值。\n",
    "\n",
    "这里我们使用PyTorch Tensors和autograd来实现两层的神经网络，那么我们不再需要手动地使用backward。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 522.6221313476562\n",
      "199 2.693244457244873\n",
      "299 0.02032787725329399\n",
      "399 0.00037950018304400146\n",
      "499 5.442695692181587e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Defining new autograd functions\n",
    "\n",
    "所有原始的autograd操作实际上是两个在Tensor上的函数。forward函数使得input Tensors生成output Tensors，backward 函数接收转换成scaler值的output Tensors的梯度下降，然后生成转换成相同scaler值的input Tensors的梯度下降。\n",
    "\n",
    "在PyTorch中我们可以很容易的定义我们自己的autograd，通过定义torch.autograd.Function的子类，以及使用forward和backward函数。\n",
    "\n",
    "我们可以使用新的autograd操作，通过建立一个instance，然后像使用函数一样，让input data通过Tensors。\n",
    "\n",
    "在以下的例子中，我们定义了定制的autograd函数来实现ReLU nonlinearity，然后在两层神经网络上使用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 325.7989807128906\n",
      "199 0.6053511500358582\n",
      "299 0.0021314057521522045\n",
      "399 8.822407107800245e-05\n",
      "499 2.3329428586293943e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow: Static Graphs\n",
    "\n",
    "PyTorch的autograd看上去有点像TensorFlow：两个框架中我们都定义了计算图，使用了自动偏微分去计算梯度下降。最大的不同是TensorFlow的计算图是静态的而PyTorch的计算图是动态的。\n",
    "\n",
    "在TensorFlow中，计算图是一次定义，多次使用，将不同的input数据输入图中。在PyTorch中，每一次forward操作都定义一个新的计算图。\n",
    "\n",
    "静态计算图是很好的，因为你可以在图之前进行优化，比如说一个框架可能决定加入某些图操作使得它更有效运行，或者想出一些静态图分发在不同GPUs或者机器上的策略。如果你重新使用相同的静态图，那么这个可能高昂的前期优化会在后期不断使用中得到摊销。\n",
    "\n",
    "静态图和动态图一个不同的方面是控制流。\n",
    "\n",
    "\n",
    "One aspect where static and dynamic graphs differ is control flow. For some models we may wish to perform different computation for each data point; for example a recurrent network might be unrolled for different numbers of time steps for each data point; this unrolling can be implemented as a loop. With a static graph the loop construct needs to be a part of the graph; for this reason TensorFlow provides operators such as tf.scan for embedding loops into the graph. With dynamic graphs the situation is simpler: since we build graphs on-the-fly for each example, we can use normal imperative flow control to perform computation that differs for each input.\n",
    "\n",
    "跟PyTorch autograd的例子对比，下面我们使用TensorFlow来拟合一个简单的两层网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e64b18d26310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Create placeholders for the input and target data; these will be filled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# with real data when we execute the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# First we set up the computational graph:\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create placeholders for the input and target data; these will be filled\n",
    "# with real data when we execute the graph.\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# Create Variables for the weights and initialize them with random data.\n",
    "# A TensorFlow Variable persists its value across executions of the graph.\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.\n",
    "# Note that this code does not actually perform any numeric operations; it\n",
    "# merely sets up the computational graph that we will later execute.\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# Compute loss using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# Compute gradient of the loss with respect to w1 and w2.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# Update the weights using gradient descent. To actually update the weights\n",
    "# we need to evaluate new_w1 and new_w2 when executing the graph. Note that\n",
    "# in TensorFlow the the act of updating the value of the weights is part of\n",
    "# the computational graph; in PyTorch this happens outside the computational\n",
    "# graph.\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# Now we have built our computational graph, so we enter a TensorFlow session to\n",
    "# actually execute the graph.\n",
    "with tf.Session() as sess:\n",
    "    # Run the graph once to initialize the Variables w1 and w2.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create numpy arrays holding the actual data for the inputs x and targets\n",
    "    # y\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    for t in range(500):\n",
    "        # Execute the graph many times. Each time it executes we want to bind\n",
    "        # x_value to x and y_value to y, specified with the feed_dict argument.\n",
    "        # Each time we execute the graph we want to compute the values for loss,\n",
    "        # new_w1, and new_w2; the values of these Tensors are returned as numpy\n",
    "        # arrays.\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                    feed_dict={x: x_value, y: y_value})\n",
    "        if t % 100 == 99:\n",
    "            print(t, loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  nn module\n",
    "###  PyTorch: nn\n",
    "\n",
    "计算图和autograd对于处理复杂运算和自动偏微分来说是很强大的工具，然而对于大型神经网络来说，原始autograd是有点太低级了。\n",
    "\n",
    "当搭建一个神经网络，我们总是想到如何安排计算进神经层，有一些具有可学习的参数，在之后会不停地优化。\n",
    "\n",
    "在TensorFlow里面，库比如说像Keras, TensorFlow-Slim, and TFLearn提供对于建造神经网络的原始计算图的高级别抽象。\n",
    "\n",
    "在PyTorch中，nn库提供同样的作用。nn库定义了一个模块，与神经网络层相当。一个模块接收input Tensors然后计算出output Tensors，同时维持一种中间态，让Tensors中包含可学习的参数。nn库同时定义了一组在训练神经网络中有用的损失函数。\n",
    "\n",
    "以下例子中我们使用nn库来实现两层神经网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3.708242893218994\n",
      "199 0.0746285617351532\n",
      "299 0.003060894785448909\n",
      "399 0.00018285629630554467\n",
      "499 1.3238582141639199e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: optim\n",
    "\n",
    "到此为止，我们已经更行了模型的权重，通过手动地加上带有可学习参数(带着torch.no_grad()或者.data来避免记录autograd的情况)的Tensors。这对于简单的优化算法如随机梯度下降来说不是个大问题，但是实际上我们经常在训练神经网络时使用更复杂的优化器，如AdaGrad, RMSProp, Adam等等。\n",
    "\n",
    "在PyTorch的优化库里，抽象了优化算法的思想，还听过了优化算法的普通实现。\n",
    "\n",
    "在这个例子中，我们会使用nn库来定义我们的模型，然后使用optim库里的Adam算法来优化我们的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 54.60757064819336\n",
      "199 0.8584431409835815\n",
      "299 0.009332788176834583\n",
      "399 9.142134513240308e-05\n",
      "499 3.3641069308032456e-07\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Custom nn Modules\n",
    "\n",
    "有时候你想指定比现在模块中的sequence更复杂的模型；这样的话，你可以定义你自己的模块，通过nn.Module子类来实现，以及利用其他模块或者其他在Tensors上的autograd定义一个可以接收input Tensors生成output Tensors的forward。\n",
    "\n",
    "下面的例子为两层神经网络定制的子类模块。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.4334070682525635\n",
      "199 0.05058251693844795\n",
      "299 0.00185418373439461\n",
      "399 8.191089727915823e-05\n",
      "499 4.00275939682615e-06\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Control Flow + Weight Sharing\n",
    "\n",
    "作为动态图和权重分享的一个例子，我们实现了一个很陌生的模型，一个全连接的ReLU神经网络，在forward时选择1到4随机数，然后使用很多隐藏层，循环利用同一组权重很多次来计算最里面的隐藏层。\n",
    "\n",
    "我们可以使用Python控制流来实现这个循环，然后我们可以通过在定义forward时重复利用同一个模型实现层之间的权重分享。\n",
    "\n",
    "我们可以简单的实现这个模型当成是模块的子类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 12.992911338806152\n",
      "199 0.784458339214325\n",
      "299 1.714671015739441\n",
      "399 0.3597424626350403\n",
      "499 0.3711857795715332\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        #super调用父类中的函数\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
