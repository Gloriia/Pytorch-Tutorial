{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn究竟是什？：What's torch.nn really?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们建议运行这份教材在notebook而不是script上。下载对应的notebook（.ipynb）文件，点击以下链接。\n",
    "\n",
    "PyTorch提供优雅设计的模块和类\n",
    "`torch.nn <https://pytorch.org/docs/stable/nn.html>`_ ,\n",
    "`torch.optim <https://pytorch.org/docs/stable/optim.html>`_ ,\n",
    "`Dataset <https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset>`_ ,\n",
    "and `DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`_\n",
    "来帮助你创建和调整神经网络。\n",
    "\n",
    "为了完全地利用他们的力量以及定制化地解决你的问题，你需要真的理解我们在做什么。为了更好地理解，我们会首先在mnist数据集上训练基础的神经网络。为了建立这种理解，我们会功能性地使用最基本的PyTorch tensor。然后，我们会一次次逐渐增加特性从\"torch.nn\",\"torch.optim\",\"Dataset\",\"DataLoader\"，展示每一块是做什么的，以及它是怎么使得代码要么更简介，要么更灵活。\n",
    "\n",
    "**这份教材假定你已经安装好了PyTorch，以及熟悉了基本的tensor操作。**\n",
    "（如果你已经熟悉了numpy数组操作，你会发现PyTorch tensor操作是差不多了。\n",
    "\n",
    "\n",
    "MNIST data setup\n",
    "----------------\n",
    "我们会使用经典的`MNIST <http://deeplearning.net/data/mnist/>`_数据集，这里面包含了手写数字0到9的黑白数据集。\n",
    "\n",
    "我们会使用 `pathlib <https://docs.python.org/3/library/pathlib.html>`_来处理路径(部分python3的标准库)，然后会使用`requests <http://docs.python-requests.org/en/master/>`_来下载数据。我们只会在我们使用模块时import它，所以你可以看到在每一步它们是怎么被使用的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is in numpy array format, and has been stored using pickle,\n",
    "a python-specific format for serializing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is 28 x 28, and is being stored as a flattened row of length\n",
    "784 (=28x28). Let's take a look at one; we need to reshape it to 2d\n",
    "first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 使用 ``torch.tensor``, 而不是numpy数组，所以我们需要去转化我们的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural net from scratch (no torch.nn)\n",
    "---------------------------------------------\n",
    "\n",
    "让我们首先使用PyTorch tensor操作创建一个模型。我们假定你已经熟悉神经网络的基本操作。（如果你不熟悉，你可以学习它们`course.fast.ai <https://course.fast.ai>`_)\n",
    "\n",
    "PyTorch 提供了创建随机或者zero-filled tensor，那里我们会为简单线性模型创建我们的权重和误差。这些只是常规tensor，带着一个非常特殊的功能：我们告诉pytorch他们需要一个梯度下降。这造成了pytorch记录所有在tensor上的操作，这样他就可以在自动反向传播中计算梯度下降。\n",
    "\n",
    "对于权重，我们在初始化之后设置了\"requires_grad\"，这样我们不会想包括了梯度下降这步。（注意到trailling\"_\"在pytorch中指定in-place表现的操作。）\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>我们用这个来初始化权重`Xavier initialisation <http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf>`_\n",
    "   (by multiplying with 1/sqrt(n)).</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感谢pytorch自动计算梯度下降的能力，我们可以像模型一样使用任何标准的python函数（或者callable的对象）！所以让只是写一个plain矩阵相乘和broadcasted addition来创建一个简单的线性模型。我们同时需要一个激活函数，这样我们会写`log_softmax`以及使用它。记得：虽然PyTorch提供很多预写好损失函数，激活函数等等，你可以使用plain来简单地写你自己的。PyTorch会甚至创建更快的GPU或者vectorized CPU代码给你函数自动化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在以上的例子里，``@``代表点乘运算，我们会call我们的函数生气每一个batch的数据(在这里，64 images)。这就是一个*forward pass*。注意到我们预测在这个阶段不会比随机预测准，因为我们就是从随机权重开始的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "preds = model(xb)  # predictions\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们看来，那个``preds``的tensor包含不只是tensor值，还有一个梯度下降函数。我们之后在反向传播中使用这个。\n",
    "\n",
    "让我们使用负log-likelihood来作为损失函数使用(再说一遍，我们可以只是使用标准python):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们检查我们随机模型的损失，这样我们可以看我们是不是在反向传播之后提高了效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们同时使用函数来计算模型的准确率。对于每一个预测，如果最大值带有的index与目标值匹配，那么预测值是正确的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们检查我们随机模型的准确性，这样我们可以看到我们的准确率是不是随着我们的损失提高。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以现在运行一个训练循环。对于每一次迭代，我们会:\n",
    "\n",
    "- 选择一个数据的mini-batch(大小是 ``bs``)\n",
    "- 使用模型来得到预测\n",
    "- 计算损失\n",
    "- ``loss.backward()`` 更新模型的梯度下降，在这个案例里，是 ``weights``和``bias``。\n",
    "\n",
    "我们现在使用这些梯度下降来更新权重和误差。我们通过使用``torch.no_grad()``文本管理来操作，因为我们不想这些行动来记录我们下一次梯度下降的计算。你可以看PyTorch's 自动梯度记录操作`here <https://pytorch.org/docs/stable/notes/autograd.html>`_。然后我们设置梯度下降到零，这样我们就准备好下一个循环了。否则，我们梯度下降会记录一个所有所发生操作的运行的tally（比如说``loss.backward()``*adds*那个梯度下降到所有被储存的，而不是替代它们）。\n",
    "\n",
    ".. tip:: 你可以使用标准的python来debugger到PyTorch的代码，允许你来检查每一步的各种变量值。取消下面的``set_trace()`` 语句来尝试。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        #         set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就是：我们从无到有地创建和训练了一个最小神经网络(在这里，是一个逻辑回归模型，因为我们没有隐藏层)。\n",
    "\n",
    "让我们检查损失和准确度和跟我们之前所有的东西比较。我们期待损失会下降以及准确率会上升，它们确实是这样的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using torch.nn.functional\n",
    "------------------------------\n",
    "\n",
    "我们现在会重构我们的代码，这样它就跟之前一样，只有我们开始利用PyTorch ``nn`` 的类来让它更加简洁和灵活。在这里的每一步，我们都应该让我们的代码多多少少地：更短，可读性更强，以及更灵活。\n",
    "\n",
    "第一步和更简单地一步是通过替代那些从``torch.nn.functional``得到的我们的手写激活函数和损失函数去让我们的代码更短(那些通常从名字空间``F``中import进来的)。这个模块包括所有在``torch.nn`` 库里的函数(然而在类里包含库的其他部分)。同时，一个更广阔的损失和激活函数，你可以同时找到一些传统创建神经网络的函数，比如pooling函数。(这里同时有传统的卷积，线性层，等等，但是正如我们所看到的，这里通常会更好地使用库里的其他部分来处理。)\n",
    "\n",
    "如果你使用负log likelihood损失和log softmax激活函数，那么Pytorch提供了一个简单的函数``F.cross_entropy``，它结合了上面两个。这样我们可以甚至把激活函数踢出我们的模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到我们不再在``model``函数里call``log_softmax``了。让我们确认我们的损失和准确度是跟之前一样的:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor using nn.Module\n",
    "-----------------------------\n",
    "接下来，我们会使用``nn.Module``和``nn.Parameter``，对于一个明确和更加简洁的训练循环。我们子类``nn.Module``(他自己是一个类和足够去跟踪状态)。在这个案例里，我们想去创建一个类，来保持住我们的权重，误差，以及前一步的方法。``nn.Module`` 有很多特性和方法(例如``.parameters()``和``.zero_grad()``)这些我们会使用的。\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``nn.Module`` (大写的M) 是一个PyTorch中专门的概念，也是一个我们会经常使用的类。\n",
    " ``nn.Module`` 是跟Python概念不一样的(小写的 ``m``) `模块 <https://docs.python.org/3/tutorial/modules.html>`_,这里是可以被导入的Python代码。</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然我们现在使用了对象而不是只是函数，我们要用模型的例子来说明:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以用之前一样的方法来计算损失。注意到``nn.Module``对象被使用就像他们是函数一样(因为他们是*callable*)，但是这之后的场景是Pytorch会自动地呼叫我们的``forward``方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前为了我们训练循环我们需要按照名字更新每一组参数，然后手动单独地为每一个参数归零的梯度下降，像这样:\n",
    "::\n",
    "\n",
    "  with torch.no_grad():\n",
    "      weights -= weights.grad * lr\n",
    "      bias -= bias.grad * lr\n",
    "      weights.grad.zero_()\n",
    "      bias.grad.zero_()\n",
    "现在我们可以利用model.parameters()和model.zero_grad() (这些都是通过``nn.Module``被PyTorch定义的)，特别地如果我们有更复杂的模型:::\n",
    "\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for p in model.parameters(): p -= p.grad * lr\n",
    "      model.zero_grad()\n",
    "\n",
    "我们会圆满地完成在``fit``函数中每一个小训练循环，这样我们可以一次又一次地运行它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们再次检查我们的损失是否变小:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor using nn.Linear\n",
    "-------------------------\n",
    "\n",
    "\n",
    "我们继续重组我们的代码。不是人工的定义和初始化``self.weights``和``self.bias``，以及计算``xb  @\n",
    "self.weights + self.bias``，我们会使用Pytorch类`nn.Linear <https://pytorch.org/docs/stable/nn.html#linear-layers>`_ 来为一个线性层，它为我们做所有的东西。Pytorch有很多种预训练层是可以很大地简化我们代码，以及常常加快我们的程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们案例化我们的模型以及用相同的方式计算损失:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们仍然可以使用跟之前相同的``fit``方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor using optim\n",
    "------------------------------\n",
    "\n",
    "Pytorch同时有各种各样优化算法的库``torch.optim``。我们可以从我们的优化器来使用``step``方法来做向前一步，而不是手动的更新每一个参数。\n",
    "\n",
    "这个会让我们替代我们原先手动的代码优化步骤:\n",
    "::\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for p in model.parameters(): p -= p.grad * lr\n",
    "      model.zero_grad()\n",
    "\n",
    "相反地我们只是使用:\n",
    "::\n",
    "  opt.step()\n",
    "  opt.zero_grad()\n",
    "\n",
    "(``optim.zero_grad()`` 重新设置梯度到0而且我们需要在计算出梯度之前call它来进行下一组minibatch。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们会定义一个小函数来创建我们的模型和优化器，这样我们可以在未来重复使用它。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor using Dataset\n",
    "------------------------------\n",
    "\n",
    "PyTorch有一个概括性的数据集类。一个数据集可以被所有的\n",
    "has an abstract Dataset class.  A Dataset can be anything that has\n",
    "a ``__len__`` function (called by Python's standard ``len`` function) and\n",
    "a ``__getitem__`` function as a way of indexing into it.\n",
    "\n",
    "\n",
    "`这份教程 <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`_通过一个很好的创建传统``FacialLandmarkDataset``类作为``Dataset``数据集的子类。\n",
    "\n",
    "\n",
    "PyTorch's `TensorDataset <https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset>`_是一个有包装器tensor的数据集。通过定义一个长度和indexing的方式，这个同时会给到我们一种循环、索引和在tensor第一纬度切片的方式，这个会让它更加简单来启动独立和非独立变量在训练时相同的line。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``x_train``和``y_train`` 可以结合到一个独立的``TensorDataset``,这会使得在切片中循环变得更简单。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前的，我们有单独为x和y值遍历的minibatch:\n",
    "::\n",
    "    xb = x_train[start_i:end_i]\n",
    "    yb = y_train[start_i:end_i]\n",
    "\n",
    "现在，我们可以通过这两步一起:\n",
    "::\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        xb, yb = train_ds[i * bs: i * bs + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor using DataLoader\n",
    "------------------------------\n",
    "\n",
    "Pytorch's ``DataLoader``是负责batch的。你可以从任何的``Dataset``来创建一个``DataLoader``。``DataLoader``让它更容易地去遍历它每一个batches。而不是必须去使用``train_ds[i*bs : i*bs+bs]``，DataLoader自动地给我们每一个minibatch。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前，我们在batches(xb,yb)上的loop循环像以下般:\n",
    "::\n",
    "      for i in range((n-1)//bs + 1):\n",
    "          xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "          pred = model(xb)\n",
    "\n",
    "现在，我们的loop循环是更干净的，正如 (xb, yb)是自动加载到data loader一样:\n",
    "::\n",
    "      for xb,yb in train_dl:\n",
    "          pred = model(xb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多亏Pytorch的``nn.Module``, ``nn.Parameter``, ``Dataset``,以及``DataLoader``,我们训练的循环是很奇妙地更小或者更简单地去理解。让我们现在尝试去增加必须的基础特征来创建实用的模型。\n",
    "\n",
    "\n",
    "Add validation\n",
    "-----------------------\n",
    "\n",
    "在section 1中，我们只是去尝试一个合理的训练循环来建立训练数据集。事实上，你**总是** 应该一个`validation set <https://www.fast.ai/2017/11/13/validation-sets/>`_,为了识别你是不是过拟合。\n",
    "\n",
    "打乱抽取的训练数据是`important <https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks>`_为了防止batches和overfitting的相关性。另一方面，验证损失会和我们是否打乱测试集保持一致。因为无故打乱是需要额外的时间的，所以无端打乱测试集是不合理的。\n",
    "\n",
    "我们会使用比训练集大一倍的一个batch size测试集。这是因为测试集不需要反向传播所以需要更少的记忆(这不需要储存梯度)。我们会利用这个来使用更大的batch size和更快地计算损失。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们会在每一个epoch后计算以及打印测试集损失。\n",
    "\n",
    "(注意到我们总是在训练前呼叫``model.train()``，以及在预测前呼叫``model.eval()``，因为这些是被诸如``nn.BatchNorm2d``的层所使用，以及 ``nn.Dropout``来保证不同阶段的合适的行为。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "\n",
    "    print(epoch, valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create fit() and get_data()\n",
    "----------------------------------\n",
    "我们会做小小的重构。因为我们经过了为每一个训练集和测试集计算损失的过程两次，让我们把它写进函数， ``loss_batch``，这样会帮我们计算每一个batch的损失。\n",
    "\n",
    "我们通过每一个训练集的优化器，以及使用它来得到反向传播。对于测试集，我们不会传递优化器，这样的方法不会实现方向传播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``fit`` 运行必要的操作来训练我们的模型以及为每一个epoch计算训练和测试集的损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``get_data`` 返回训练集和测试集的dataloaders。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们整个过程都是获取loaders和fitting模型，我们可以通过三行代码实现:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以使用这基础的三行代码来训练各种各样的模型。\n",
    "让我们看下我们怎么用它们来训练卷积神经网络(CNN)!\n",
    "\n",
    "Switch to CNN\n",
    "-------------\n",
    "我们现在准备用三个卷积层来建立我们的神经网络。因为之前没有这样假定任何模型形式的函数，我们能够不做任何修改地使用它们来训练CNN。\n",
    "\n",
    "我们会使用Pytorch的预定义\n",
    "`Conv2d <https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d>`_ 类作为我们的卷基层。我们用三层卷基层来定义CNN。\n",
    "每一个卷积层都接着ReLU。在最后，我们实现一个average pooling。(注意到``view``是PyTorch版本的numpy``reshape``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Momentum <https://cs231n.github.io/neural-networks-3/#sgd>`_是一个随机梯度下降的变化率，这让之前的更新都即入内，它会逐渐地使训练过程变得越来越快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_CNN()\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential\n",
    "------------------------\n",
    "\n",
    "``torch.nn`` 有另外一个更加有用的类我们可以使用来简化我们的代码:`Sequential <https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential>`_ 。\n",
    "一个``Sequential`` 对象在运行每一个在它里面的模块时，都是按顺序的。这是写我们神经网络简单的方法。\n",
    "\n",
    "为了利用这个，我们需要通过一个给定的函数简单地定义一个**custom layer**。例如，PyTorch没有`view`层，我们需要为我们的神经网络创建一个。\n",
    "``Lambda``会创建一个我们可以使用的用``Sequential``定义神经网络层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个模型创建了简单的``Sequential``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping DataLoader\n",
    "-----------------------------\n",
    "\n",
    "Our CNN is fairly concise, but it only works with MNIST, because:\n",
    " - It assumes the input is a 28\\*28 long vector\n",
    " - It assumes that the final CNN grid size is 4\\*4 (since that's the average\n",
    "pooling kernel size we used)\n",
    "\n",
    "Let's get rid of these two assumptions, so our model works with any 2d\n",
    "single channel image. First, we can remove the initial Lambda layer but\n",
    "moving the data preprocessing into a generator:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can replace ``nn.AvgPool2d`` with ``nn.AdaptiveAvgPool2d``, which\n",
    "allows us to define the size of the *output* tensor we want, rather than\n",
    "the *input* tensor we have. As a result, our model will work with any\n",
    "size input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your GPU\n",
    "---------------\n",
    "\n",
    "If you're lucky enough to have access to a CUDA-capable GPU (you can\n",
    "rent one for about $0.50/hour from most cloud providers) you can\n",
    "use it to speed up your code. First check that your GPU is working in\n",
    "Pytorch:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then create a device object for it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update ``preprocess`` to move batches to the GPU:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can move our model to the GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find it runs faster now:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closing thoughts\n",
    "-----------------\n",
    "\n",
    "现在你有了数据pipeline和很多种pytorch模型的训练循环。来看看简单的模型训练可以如何实现，请看'mnist_sample'的样本notebook。\n",
    "\n",
    "当然，你可以往里面添加很多东西，例如数据增强，调参数，监控训练过程，迁移学习等等。这些特性在fastai中都可以使用，这可以跟这份教材一样使用相同的训练方法，为使用者进一步提高他们的模型提供下一步。\n",
    "\n",
    "我们在这份教材一开始都承诺了我们会解释\"torch.nn\",\"torch optim\",\"Dataset\"和\"DataLoader\"的例子。因此让我们一起来总结我们所看过的：\n",
    "\n",
    "\n",
    "\n",
    " - **torch.nn**\n",
    "\n",
    "   + ``Module``: 创建一个表现得像是函数的callable，但是可以同时包含state（比如说神经网络层权重）。它知道自己包含了什么\"Parameter\"(s)，而且可以把它所有的梯度归零，不断地循环来进行权重更新等等。\n",
    "   + ``Parameter``: 一个tensor的装饰器，它告诉\"Module\"在backprop时它用权重需要升级。只有带有\"requires_grad\"的tensor特性设置是更新的。   \n",
    "   + ``functional``: 一个模块（通常import进\"F\"名字空间）其中包括激活函数，损失函数等等，同时non-stateful版本的layers例如卷积和线性层。\n",
    " - ``torch.optim``: 包含例如是\"SGD\"的优化器，其中在backward步骤中更新了\"Parameter\"的权重。\n",
    " - ``Dataset``:一个带有\"__len__\"和\"__getitem__\"的简要的对象界面，包括用Pytorch提供的例如是\"TensorDataset\"的类／\n",
    " - ``DataLoader``: 拿任意\"Dataset\"和创建一个返回batchs of data的循环器。\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
